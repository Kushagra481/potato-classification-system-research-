{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6978893,"sourceType":"datasetVersion","datasetId":4010347}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Install necessary packages","metadata":{"id":"aiL5ryEGryF7","outputId":"e40e9c65-db2b-428f-e535-116417cfed13"}},{"cell_type":"code","source":"!pip install torchsampler\n!pip install torchmetrics\n!pip install split-folders","metadata":{"id":"vdMIT3fF2GoK","executionInfo":{"status":"ok","timestamp":1730090638763,"user_tz":-330,"elapsed":10203,"user":{"displayName":"Varun Menon","userId":"16749211396894445614"}},"outputId":"03a2b877-57ca-491c-b8e3-cecb62b356c0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#  2. Import required libraries","metadata":{"id":"vnN5WLYW2GoL"}},{"cell_type":"code","source":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image\nimport torchvision.models as models\nfrom torch.cuda.amp import autocast, GradScaler\nfrom tqdm.autonotebook import tqdm, trange\nimport time\nfrom sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,confusion_matrix, classification_report\nfrom torchsampler import ImbalancedDatasetSampler\nfrom torch.utils.data.sampler import BatchSampler\nimport pandas as pd\nimport random\nimport torch\nfrom torch.autograd import Variable\nfrom torch.nn import Linear, ReLU, LeakyReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n!jupyter nbextension enable --py widgetsnbextension\n!jupyter nbextension install --py widgetsnbextension","metadata":{"id":"GEPF5Hprr1k1","outputId":"5b1095da-32d6-465f-8e1e-5ee8e2209460","executionInfo":{"status":"error","timestamp":1730094107195,"user_tz":-330,"elapsed":15065,"user":{"displayName":"Varun Menon","userId":"16749211396894445614"}},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Data Pre-processing","metadata":{"id":"to0LoSj62GoM"}},{"cell_type":"code","source":"import splitfolders\n\n# Split the dataset into train, test, and val folders\nsplitfolders.ratio('/kaggle/input/potato-leaf-disease-dataset/Potato Leaf Disease Dataset in Uncontrolled Environment', output=\"data1\", seed=1337, ratio=(.7, 0.2, 0.1))\n\n# Define the paths to train, test, and val folders\ntrain_folder = \"data1/train\"\ntest_folder = \"data1/test\"\nval_folder = \"data1/val\"\n","metadata":{"id":"aPyKTCiJ2GoM","executionInfo":{"status":"error","timestamp":1730094085568,"user_tz":-330,"elapsed":831,"user":{"displayName":"Varun Menon","userId":"16749211396894445614"}},"outputId":"7c258e6b-faaa-406e-b9fa-c6868fa820b3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Data Augmentation**","metadata":{"id":"sk6V0zjt2GoM"}},{"cell_type":"code","source":"\ntrain_transforms = transforms.Compose([\n                                       transforms.Resize((256,256)),\n                                       transforms.RandomHorizontalFlip(),\n                                       transforms.RandomRotation(90),\n                                       transforms.ToTensor(),\n\n])\n\nval_transforms = transforms.Compose([\n                                       transforms.Resize((256,256)),\n                                       transforms.RandomHorizontalFlip(),\n                                       transforms.RandomRotation(90),\n                                       transforms.ToTensor(),\n\n])\n\ntest_transforms = transforms.Compose([\n                                       transforms.Resize((256,256)),\n                                       transforms.ToTensor(),\n\n])","metadata":{"id":"p5FduGFbUH64","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **4. Create data loaders**","metadata":{"id":"FEzNUtCP2GoM"}},{"cell_type":"code","source":"train_dataset = torchvision.datasets.ImageFolder(root = train_folder, transform=train_transforms)\nval_dataset = torchvision.datasets.ImageFolder(root = val_folder, transform=val_transforms)\ntest_dataset = torchvision.datasets.ImageFolder(root = test_folder, transform=test_transforms)","metadata":{"id":"CcP3xZGQUK69","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(dataset = train_dataset, sampler=ImbalancedDatasetSampler(train_dataset), batch_size=16)\nval_loader = torch.utils.data.DataLoader(dataset = val_dataset, batch_size=16, shuffle=False)\ntest_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size=16, shuffle=False)","metadata":{"id":"ncQxo4bFUQVq","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"id":"FgoSUGEzUUKF","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **5. Model definition**","metadata":{"id":"xDkGjA4z2GoN"}},{"cell_type":"code","source":"model = models.alexnet(pretrained=True)\nprint(model)","metadata":{"id":"WoTS8vMjUXew","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **6. Modifying the Classifier**","metadata":{"id":"FKG_If6E2GoN"}},{"cell_type":"code","source":"num_ftrs = model.classifier[6].in_features\nmodel.classifier[6] =  nn.Linear(num_ftrs, 7)","metadata":{"id":"jQwtecEgVIAd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **7. Loss function and optimizer**","metadata":{"id":"tZfSZnn22GoN"}},{"cell_type":"code","source":"\nif torch.cuda.is_available() == True:\n    model = model.cuda()\nelse:\n    model = model\ncriterion = torch.nn.CrossEntropyLoss()\n\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nscaler = GradScaler(enabled=True)\nuse_cuda = True","metadata":{"id":"52qYFHQlVNoM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **8. Training and testing functions**","metadata":{"id":"8ZCFfKF82GoO"}},{"cell_type":"code","source":"train_accu = []\ntraining_loss = []\n\ndef train(model, iterator, optimizer, criterion, device):\n\n    epoch_loss = 0\n    epoch_acc = 0\n    image_preds_all = []\n    image_targets_all = []\n\n    model.train()\n\n    for (x, y) in tqdm(iterator, desc=\"Training\", leave=False):\n\n        x = x.to(device).float()\n        y = y.to(device).long()\n\n        with autocast():\n\n            y_pred = model(x)\n\n            image_preds_all += [torch.argmax(y_pred, 1).detach().cpu().numpy()]\n            image_targets_all += [y.detach().cpu().numpy()]\n\n            loss = criterion(y_pred, y)\n\n            acc = calculate_accuracy(y_pred, y)\n\n            scaler.scale(loss).backward()\n            #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n    image_preds_all = np.concatenate(image_preds_all)\n    image_targets_all = np.concatenate(image_targets_all)\n    score = (image_preds_all==image_targets_all).mean()\n\n    train_losss = epoch_loss / len(iterator)\n\n    train_accu.append(score*100)\n    training_loss.append(train_losss)\n\n    #print(score)\n    #print(len(train_accu), len(training_loss))\n\n    return epoch_loss / len(iterator), epoch_acc / len(iterator), train_accu, training_loss","metadata":{"id":"fFHAPeLoVWEh","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_accu = []\neval_loss = []\n\ndef evaluate(model, iterator, criterion, device):\n\n    epoch_loss = 0\n    epoch_acc = 0\n    image_preds_all = []\n    image_targets_all = []\n\n    model.eval()\n\n    with torch.no_grad():\n\n        for (x, y) in tqdm(iterator, desc=\"Evaluating\", leave=False):\n\n            x = x.to(device).float()\n            y = y.to(device).long()\n\n            y_pred = model(x)\n\n            image_preds_all += [torch.argmax(y_pred, 1).detach().cpu().numpy()]\n            image_targets_all += [y.detach().cpu().numpy()]\n\n            loss = criterion(y_pred, y)\n\n            acc = calculate_accuracy(y_pred, y)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n\n    image_preds_all = np.concatenate(image_preds_all)\n    image_targets_all = np.concatenate(image_targets_all)\n    score = (image_preds_all==image_targets_all).mean()\n\n    val_losss = epoch_loss / len(iterator)\n\n    val_accu.append(score*100)\n    eval_loss.append(val_losss)\n\n    performance_matrix(image_targets_all, image_preds_all)\n\n    return epoch_loss / len(iterator), epoch_acc / len(iterator), val_accu, eval_loss","metadata":{"id":"l9bxrvlEVY0Z","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def performance_matrix(true,pred):\n    precision = precision_score(true,pred,average='macro')\n    recall = recall_score(true,pred,average='macro')\n    accuracy = accuracy_score(true,pred)\n    f1_sco = f1_score(true,pred,average='macro')\n    print('Precision: {:.4f} Recall: {:.4f}, Accuracy: {:.4f}: ,f1_score: {:.4f}'.format(precision,recall,accuracy,f1_sco))\n    print('Classification Report:\\n',classification_report(true, pred))","metadata":{"id":"QTgu-yll2GoO","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_accuracy(y_pred, y):\n    top_pred = y_pred.argmax(1, keepdim=True)\n    correct = top_pred.eq(y.view_as(top_pred)).sum()\n    acc = correct.float() / y.shape[0]\n    return acc","metadata":{"id":"30FyLWx-2GoO","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{"id":"dEOVgoaXVc7c","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def checkpoint_model(epoch, model, opt, best_val_acc, model_path):\n    model_state_dict = model.state_dict() if (device.type == 'cuda') else model.state_dict()\n    torch.save({\n        'epoch': epoch,\n        'model_state_dict': model_state_dict,\n        'opt_state_dict': opt.state_dict(),\n        'best_val_acc': best_val_acc\n    }, model_path)","metadata":{"id":"XtuWwUiTVfzP","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_ckp(checkpoint_fpath, model, optimizer):\n    checkpoint = torch.load(checkpoint_fpath)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['opt_state_dict'])\n    return model, optimizer, checkpoint['epoch']","metadata":{"id":"Y-_eiY2rVlNW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **9. Training and validating the model**","metadata":{"id":"Wu8AXYib2GoO"}},{"cell_type":"markdown","source":"EPOCHS = 50\n\nbest_valid_loss = float('inf')\nbest_val_acc = 0.\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')\n\ntrain_acc_gr = []\ntrain_loss_gr = []\nval_acc_gr = []\nval_loss_gr = []\n\nfor epoch in trange(EPOCHS, desc=\"Epochs\"):\n\n    start_time = time.monotonic()\n\n    train_loss, train_acc, train_acc_gr, train_loss_gr = train(model, train_loader, optimizer, criterion, device)\n    valid_loss, valid_acc, val_acc_gr, val_loss_gr = evaluate(model, val_loader, criterion, device)\n\n    if epoch % 30 == 0:\n        checkpoint_model(epoch, model, optimizer, best_val_acc, '/kaggle/working/CNN_epoch%d.pth' % epoch)\n\n\n    end_time = time.monotonic()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n\n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n\n\nplt.plot(train_acc_gr,'-o')\nplt.plot(val_acc_gr,'-o')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(['Train','Valid'])\nplt.title('Train vs Valid Accuracy')\nplt.show()\n\nplt.plot(train_loss_gr,'-o')\nplt.plot(val_loss_gr,'-o')\nplt.xlabel('epoch')\nplt.ylabel('losses')\nplt.legend(['Train','Valid'])\nplt.title('Train vs Valid Losses')\nplt.show()","metadata":{"id":"XczC7kpKVm-a","_kg_hide-output":true}},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tqdm import trange\n\nEPOCHS = 20\nPATIENCE = 10  # Number of epochs to wait before early stopping\nBATCH_SIZE = 16\nbest_valid_loss = float('inf')\nbest_val_acc = 0.\nepochs_no_improve = 0  # Counter for early stopping\n\n# 1. **Count Parameters in AlexNet**\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')\n\ntrain_acc_gr = []\ntrain_loss_gr = []\nval_acc_gr = []\nval_loss_gr = []\n\n# 2. **Optimizing Learning Rate and Weight Decay**\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n\n# 3. **Learning Rate Scheduler**\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n\n# 4. **Fine-Tune Selected Layers (if using a pretrained AlexNet)**\nfor param in model.parameters():\n    param.requires_grad = False  # Freeze all layers initially\n\n# Unfreeze specific layers for fine-tuning\nfor param in model.classifier[6].parameters():\n    param.requires_grad = True\n\n# Training Loop with Early Stopping\nfor epoch in trange(EPOCHS, desc=\"Epochs\"):\n    start_time = time.monotonic()\n\n    # 5. **Training Step**\n    train_loss, train_acc, train_acc_gr, train_loss_gr = train(model, train_loader, optimizer, criterion, device)\n    \n    # 6. **Validation Step**\n    valid_loss, valid_acc, val_acc_gr, val_loss_gr = evaluate(model, val_loader, criterion, device)\n    \n    # Update scheduler based on validation loss\n    scheduler.step(valid_loss)\n\n    # Check for improvement\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        best_val_acc = valid_acc\n        epochs_no_improve = 0  # Reset counter if validation improves\n    else:\n        epochs_no_improve += 1\n\n    # Early stopping check\n    if epochs_no_improve >= PATIENCE:\n        print(f'Early stopping at epoch {epoch+1} due to no improvement in validation loss/accuracy.')\n        break\n\n    # Save model checkpoint every 30 epochs\n    if epoch % 30 == 0:\n        checkpoint_model(epoch, model, optimizer, best_val_acc, '/kaggle/working/CNN_epoch%d.pth' % epoch)\n\n    end_time = time.monotonic()\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n\n    # Print metrics for each epoch\n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n\n# Plot training and validation accuracy and loss\nplt.plot(train_acc_gr, '-o')\nplt.plot(val_acc_gr, '-o')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend(['Train', 'Valid'])\nplt.title('Train vs Valid Accuracy')\nplt.show()\n\nplt.plot(train_loss_gr, '-o')\nplt.plot(val_loss_gr, '-o')\nplt.xlabel('epoch')\nplt.ylabel('losses')\nplt.legend(['Train', 'Valid'])\nplt.title('Train vs Valid Losses')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **10. Testing the model**","metadata":{"id":"Tdhk9UZD2GoP"}},{"cell_type":"code","source":"epoch_loss = 0\nepoch_acc = 0\nimage_preds_all = []\nimage_targets_all = []\n\nmodel.eval()\n\nwith torch.no_grad():\n\n    for (x, y) in tqdm(val_loader, desc=\"Evaluating\", leave=False):\n\n        x = x.to(device).float()\n        y = y.to(device).long()\n\n        y_pred = model(x)\n\n        image_preds_all += [torch.argmax(y_pred, 1).detach().cpu().numpy()]\n        image_targets_all += [y.detach().cpu().numpy()]\n\n        loss = criterion(y_pred, y)\n\n        acc = calculate_accuracy(y_pred, y)\n\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n\nimage_preds_all = np.concatenate(image_preds_all)\nimage_targets_all = np.concatenate(image_targets_all)\nscore = (image_preds_all==image_targets_all).mean()\n\nperformance_matrix(image_targets_all, image_preds_all)","metadata":{"id":"8nCiFEl4WzvE","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n\ndef predict_image(image_path, model, transform, device):\n    # Load the image\n    image = Image.open(image_path).convert('RGB')\n\n    # Apply the same transformations as used during training\n    image = transform(image).unsqueeze(0)  # Add batch dimension\n\n    # Move the image to the device (GPU/CPU)\n    image = image.to(device)\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Make prediction without calculating gradients\n    with torch.no_grad():\n        # Get the model's output\n        output = model(image)\n\n        # Get the predicted class\n        _, predicted_class = torch.max(output, 1)\n\n    # Return the predicted class\n    return predicted_class.item()\n\n# Example usage:\nimage_path = '/kaggle/input/potato-leaf-disease-dataset/Potato Leaf Disease Dataset in Uncontrolled Environment/Phytopthora/1692335404516.jpg'  # Replace with the path to the image you want to predict\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor()\n])\n\npredicted_class = predict_image(image_path, model, transform, device)\nif predicted_class==0:\n    print(f'The predicted class for the given image is: bacteria')\nelif predicted_class==1:\n    print(f'The predicted class for the given image is: fungi')\nelif predicted_class==2:\n    print(f'The predicted class for the given image is: healthy')\nelif predicted_class==3:\n    print(f'The predicted class for the given image is: Nemastode')\nelif predicted_class==4:\n    print(f'The predicted class for the given image is: Pest')\nelif predicted_class==5:\n    print(f'The predicted class for the given image is: Phytopthora')\nelif predicted_class==6:\n    print(f'The predicted class for the given image is: Virus')\n","metadata":{"id":"T-FO2ALNK61v","trusted":true},"outputs":[],"execution_count":null}]}